{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZRLAFs1GQNA"
   },
   "source": [
    "Author: Wolfgang Black <br>\n",
    "date_modified: 2022-07-24\n",
    "    \n",
    "# Notebook purpose:\n",
    "    \n",
    "This notebook is meant to be run in google colab inside a /test/ folder placed at the same level as /src/. Its meant to do a few tests on the training data, data artificts, and the config generated via updated_config.py\n",
    "\n",
    "This nb is responsible for debugging /test/test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmivHokqFPLU",
    "outputId": "06eed495-2c4d-46f7-f89e-d79799aedd9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pytest\n",
    "from collections import Counter\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "base_dir = 'drive/MyDrive/Colab Notebooks/ProtCNN/src/'\n",
    "os.chdir(base_dir)\n",
    "from utils.datautils import *\n",
    "from utils.modelutils import *\n",
    "# os.chdir('../test/')\n",
    "\n",
    "# from utils.datautils import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mI-V5SlZKlew",
    "outputId": "dd5506eb-0771-4536-d84c-35326e303174"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_config': {'DATA_DIR': './../../../PFAM_database/data/random_split/', 'MAX_LEN': 120, 'TEST_BATCH_SIZE': 100, 'TEST_RANGE': [0, 10000]}, 'model_config': {'NUM_MODELS': 1, 'MODEL_DIR': '../models/', 'NUM_RES_BLOCKS': 3, 'FILTERS': 128, 'D_RATE': [2, 3, 3], 'MAX_LEN': 120, 'DROPOUT': 0.5, 'L2_FACTOR': 0.0001, 'OPT': 'adam', 'LOSS': 'sparse_categorical_crossentropy', 'METRICS': ['accuracy'], 'EPOCHS': 5, 'BATCH_SIZE': 256}}\n"
     ]
    }
   ],
   "source": [
    "with open('./config/config.json') as json_data_file:\n",
    "  config = json.load(json_data_file)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_hEKMZgKoEY6",
    "outputId": "d5e1ab01-0c95-405a-ce98-a5c3ec992e41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building artifacts for training \n",
      "\n",
      "There are 17930 labels. \n",
      "\n",
      "AA dictionary formed. the length of dictionary is: 21. \n",
      "\n",
      "building dataset dictionaries \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"building artifacts for training\",'\\n')\n",
    "\n",
    "train_data, train_targets = reader('train',config['data_config']['DATA_DIR'])\n",
    "fam2label = build_labels(train_targets)\n",
    "word2id = build_vocab(train_data)\n",
    "\n",
    "print('building dataset dictionaries','\\n')\n",
    "##call class to get dictionaries\n",
    "train_data = SequenceData(word2id, fam2label, config['data_config']['MAX_LEN'], config['data_config']['DATA_DIR'],\"train\")\n",
    "train_dict = train_data.get_data_dictionaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "A9DzvbbLoAAI"
   },
   "outputs": [],
   "source": [
    "def verify_labels_in_encoded_values(y: list, fam2label:dict):\n",
    "  \"\"\"This test verifies there are no extraneous values in the labels that do not exist in the label encoder\"\"\"\n",
    "  assert set(list(Counter(y).keys())).issubset(list(fam2label.values())) == True, 'verify_labels_in_encoded_values FAILED - there are labels in y that dne in the encoder'\n",
    "  print('verify_labels_in_encoded_values PASSED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dShRwuroVek",
    "outputId": "0b016518-ff29-442b-cc8d-0aed1f19a641"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verify_labels_in_encoded_values PASSED\n"
     ]
    }
   ],
   "source": [
    "verify_labels_in_encoded_values(train_dict['target'], fam2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MLVIFqs_olFD"
   },
   "outputs": [],
   "source": [
    "def verify_unique_labels(y: list, fam2labels: dict):\n",
    "    \"\"\"This verifies that the training data doesn't have more unique values than exists in the label encoder\"\"\"\n",
    "    counter_object = Counter(y)\n",
    "    keys = counter_object.keys()\n",
    "    assert len(fam2labels) >= len(keys), 'failed, the number unique labels in training is greater than the expected number'\n",
    "    print('verify_unique_labels PASSED: There are not more labels encoded in the data than are possible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wGA5_T4ypmHJ",
    "outputId": "0b87c546-0f37-4037-d926-4558e92d1687"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verify_unique_labels PASSED: There are not more labels encoded in the data than are possible\n"
     ]
    }
   ],
   "source": [
    "verify_unique_labels(train_dict['target'], fam2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JR9IVppYqZkE"
   },
   "outputs": [],
   "source": [
    "def verify_feature_shapes(x: dict, max_len,word2id):\n",
    "    \"\"\"This test verifies that the feature sets are the expected size\"\"\"\n",
    "    assert x['sequence'].shape[1:] == (max_len, len(word2id)), 'verified_feature_shapes failed - the shape of the features did not equal = (max_len,len(word2id))'\n",
    "    print('Passed! Feature shape and configs for model Input layer are the same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2IiTaLvIp14s",
    "outputId": "a1db4812-7ff6-4c86-cd6a-7afcfd94d5db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed! Feature shape and configs for model Input layer are the same\n"
     ]
    }
   ],
   "source": [
    "verify_feature_shapes(train_dict, config['data_config']['MAX_LEN'],word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5EXxeXZjqrVs"
   },
   "outputs": [],
   "source": [
    "def check_config_dtypes(config):\n",
    "  count = 0\n",
    "  broken_configs = []\n",
    "  if isinstance(config['data_config']['DATA_DIR'],str) == True:\n",
    "    pass\n",
    "  else:\n",
    "    count += 1\n",
    "    broken_configs.append('\\n data_config:DATA_DIR')\n",
    "\n",
    "  if isinstance(config['data_config']['MAX_LEN'],int) == True:\n",
    "    pass\n",
    "  else:\n",
    "    count += 1\n",
    "    broken_configs.append('\\n data_config:MAX_LEN')\n",
    "\n",
    "  if isinstance(config['data_config']['TEST_BATCH_SIZE'],int) == True:\n",
    "    pass\n",
    "  else:\n",
    "    count += 1\n",
    "    broken_configs.append('\\n data_config:TEST_BATCH_SIZE')\n",
    "\n",
    "  if isinstance(config['data_config']['TEST_RANGE'],list) == True:\n",
    "    pass\n",
    "  else:\n",
    "    count += 1\n",
    "    broken_configs.append('\\n data_config:BATCH_SIZE')\n",
    "\n",
    "  if isinstance(config['model_config']['BATCH_SIZE'],int) == True:\n",
    "    pass\n",
    "  else:\n",
    "    count += 1\n",
    "    broken_configs.append('\\n model_config:BATCH_SIZE')\n",
    "\n",
    "  if isinstance(config['model_config']['DROPOUT'],float) == True:\n",
    "    pass\n",
    "  else:\n",
    "    count += 1\n",
    "    broken_configs.append('\\n model_config:DROPOUT')\n",
    "\n",
    "  if isinstance(config['model_config']['D_RATE'],list) == True:\n",
    "    pass\n",
    "  else:\n",
    "    count += 1\n",
    "    broken_configs.append('\\n model_config:D_RATE')\n",
    "\n",
    "  if isinstance(config['model_config']['EPOCHS'],int) == True:\n",
    "    pass\n",
    "  else:\n",
    "    count += 1\n",
    "    broken_configs.append('\\n model_config:EPOCHS')\n",
    "\n",
    "  if isinstance(config['model_config']['FILTERS'],int) == True:\n",
    "    pass\n",
    "  else:\n",
    "    count += 1\n",
    "    broken_configs.append('\\n model_config:BATCH_SIZE')\n",
    "\n",
    "  if isinstance(config['model_config']['L2_FACTOR'],float) == True:\n",
    "    pass\n",
    "  else:\n",
    "    count += 1\n",
    "    broken_configs.append('\\n model_config:L2_FACTOR')\n",
    "    \n",
    "  if isinstance(config['model_config']['LOSS'],str) == True:\n",
    "    pass\n",
    "  else:\n",
    "    count += 1\n",
    "    broken_configs.append('\\n model_config:LOSS')\n",
    "\n",
    "  if isinstance(config['model_config']['MAX_LEN'],int) == True:\n",
    "    pass\n",
    "  else:\n",
    "    count += 1\n",
    "    broken_configs.append('\\n model_config:MAX_LEN')\n",
    "\n",
    "  if isinstance(config['model_config']['METRICS'],list) == True:\n",
    "    pass\n",
    "  else:\n",
    "    count += 1\n",
    "    broken_configs.append('\\n model_config:METRICS')\n",
    "\n",
    "  if isinstance(config['model_config']['MODEL_DIR'],str) == True:\n",
    "    pass\n",
    "  else:\n",
    "    count += 1\n",
    "    broken_configs.append('\\n model_config:MODEL_DIR')\n",
    "\n",
    "  if isinstance(config['model_config']['NUM_MODELS'],int) == True:\n",
    "    pass\n",
    "  else:\n",
    "    count += 1\n",
    "    broken_configs.append('\\n model_config:NUM_MODELS')\n",
    "\n",
    "  if isinstance(config['model_config']['NUM_RES_BLOCKS'],int) == True:\n",
    "    pass\n",
    "  else:\n",
    "    count += 1\n",
    "    broken_configs.append('\\n model_config:NUM_RES_BLOCKS')\n",
    "\n",
    "  if isinstance(config['model_config']['OPT'],str) == True:\n",
    "    pass\n",
    "  else:\n",
    "    count += 1\n",
    "    broken_configs.append('\\n model_config:OPT')\n",
    "  broken_configs.append('\\n')\n",
    "  try:\n",
    "    assert count == 0\n",
    "    print('Config passes dtype test')\n",
    "  except:\n",
    "    print('config is broken, see following list')\n",
    "    print(broken_configs)    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XLCRijrMsHIz",
    "outputId": "60b435e1-99d1-402b-84d2-5d824ae0af7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config passes dtype test\n"
     ]
    }
   ],
   "source": [
    "check_config_dtypes(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLJL8Rkhse2p"
   },
   "outputs": [],
   "source": [
    "len(D_RATE) == NUM_RES_BLOCKS\n",
    "len(TEST_RANGE) == 0 or 2\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "build_test_nb.py",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
